@misc{yu2025thinklargelanguagemodels,
      title={THiNK: Can Large Language Models Think-aloud?}, 
      author={Yongan Yu and Mengqian Wu and Yiran Lin and Nikki G. Lobczowski},
      year={2025},
	  month=june,
	  abstract = {Assessing higher-order thinking skills in large language models (LLMs) remains a fundamental challenge, especially in tasks that go beyond surface-level accuracy. In this work, we propose THiNK (Testing Higher-order Notion of Knowledge), a multi-agent, feedback-driven evaluation framework grounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative task of problem generation, critique, and revision, encouraging LLMs to think-aloud through step-by-step reflection and refinement. This enables a systematic evaluation of both lower-order (e.g., remember, understand) and higher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven state-of-the-art LLMs and perform a detailed cognitive analysis of their outputs. Results reveal that while models reliably perform lower-order categories well, they struggle with applying knowledge in realistic contexts and exhibit limited abstraction. Structured feedback loops significantly improve reasoning performance, particularly in higher-order thinking. Qualitative evaluations further confirm that THiNK-guided outputs better align with domain logic and problem structure. The code of our framework provides a scalable methodology for probing and enhancing LLM reasoning, offering new directions for evaluation grounded in learning science, which is available at our GitHub repository.},
      arxiv={2505.20184},
	  pdf={think.pdf},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
	  preview={think.png},
      url={https://arxiv.org/abs/2505.20184}, 
}

@misc{yu2025wximpactbenchdisruptiveweatherimpact,
      title={WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models}, 
      author={Yongan Yu and Qingchen Hu and Xianda Du and Jiayin Wang and Fengran Mo and Renee Sieber},
      year={2025},
	  month=may,
	  abstract = {Climate change adaptation requires the understanding of disruptive weather impacts on society, where large language models (LLMs) might be applicable. However, their effectiveness is under-explored due to the difficulty of high-quality corpus collection and the lack of available benchmarks. The climate-related events stored in regional newspapers record how communities adapted and recovered from disasters. However, the processing of the original corpus is non-trivial. In this study, we first develop a disruptive weather impact dataset with a four-stage well-crafted construction pipeline. Then, we propose WXImpactBench, the first benchmark for evaluating the capacity of LLMs on disruptive weather impacts. The benchmark involves two evaluation tasks, multi-label classification and ranking-based question answering. Extensive experiments on evaluating a set of LLMs provide first-hand analysis of the challenges in developing disruptive weather impact understanding and climate change adaptation systems. The constructed dataset and the code for the evaluation framework are available to help society protect against vulnerabilities from disasters.},
      arxiv={2505.20249},
	  pdf={wximpactbench},
	  preview={wximpactbench.png},
	  publisher = {arXiv},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.20249}, 
}

@misc{peking_codeflow,
      title={CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation}, 
      author={Sizhe Wang and Zhengren Wang and Dongsheng Ma and Yongan Yu and Rui Ling and Zhiyu Li and Feiyu Xiong and Wentao Zhang},
      year={2025},
	  abstract = {Modern software development demands code that is maintainable, testable, and scalable by organizing the implementation into modular components with iterative reuse of existing codes. We formalize this iterative, multi-turn paradigm as codeflow and introduce CodeFlowBench, the first benchmark designed to comprehensively evaluate LLMs' ability to perform codeflow, namely implementing new functionality by reusing existing functions over multiple turns. CodeFlowBench comprises 5,258 problems from Codeforces and is continuously updated via an automated pipeline, which decomposes each problem into subproblems with unit tests based on dependency tree analysis and dataflow analysis. We further propose a novel evaluation framework featured dual assessment protocol and structural metrics derived from dependency trees. Extensive experiments on 16 popular LLMs reveal significant performance degradation in multi-turn scenarios. For instance, o1-mini retains only 20.8% Pass@1 in multi-turn scenario versus 37.8% in single-turn scenario. More fine-grained analysis illustrates that model performance inversely correlates with dependency complexity. These findings not only highlight the critical challenges for supporting real-world workflows, but also establish CodeFlowBench as an essential tool for advancing code generation research.},
      archivePrefix={arXiv},
	  arxiv = {2504.21751},
	  publisher = {arXiv},
      primaryClass={cs.SE},
	  pdf={codeflow.pdf},
	  preview={codeflow.png},
      url={https://arxiv.org/abs/2504.21751},
	  selected = {true}, 
}

@misc{peking_maintaincoder,
      title={MaintainCoder: Maintainable Code Generation Under Dynamic Requirements}, 
      author={*Zhengren Wang and *Rui Ling and *Chufan Wang and *Yongan Yu and Sizhe Wang and Zhiyu Li and Feiyu Xiong and Wentao Zhang},
      month = apr,
	  year={2025},
	  abstract = {Modern code generation has made significant strides in functional correctness and execution efficiency. However, these systems often overlook a critical dimension in real-world software development: \textit{maintainability}. To handle dynamic requirements with minimal rework, we propose \textbf{MaintainCoder} as a pioneering solution. It integrates the Waterfall model, design patterns, and multi-agent collaboration to systematically enhance cohesion, reduce coupling, achieving clear responsibility boundaries and better maintainability. We also introduce \textbf{MaintainBench}, a benchmark comprising requirement changes and novel dynamic metrics on maintenance efforts. Experiments demonstrate that existing code generation methods struggle to meet maintainability standards when requirements evolve. In contrast, MaintainCoder improves dynamic maintainability metrics by more than 60\% with even higher correctness of initial codes. Furthermore, while static metrics fail to accurately reflect maintainability and even contradict each other, our proposed dynamic metrics exhibit high consistency. Our work not only provides the foundation for maintainable code generation, but also highlights the need for more realistic and comprehensive code generation research.},
	  language = {en},
	  pdf={maintaincoder.pdf},
	  arxiv = {2503.24260},
	  publisher = {arXiv},
	  preview={maintainbench.png},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2503.24260}, 
	  selected = {true},
}


@article{yu2025recall,
	title={From Recall to Reasoning: Automated Question Generation for Deeper Math Learning through Large Language Models},
	author={Yu, Yongan and Krantz, Alexandre and Lobczowski, Nikki G},
	abstract = {Educators have started to turn to Generative AI (GenAI) to help create new course content, but little is known about how they should do so. In this project, we investigated the first steps for optimizing content creation for advanced math. In particular, we looked at the ability of GenAI to produce high-quality practice problems that are relevant to the course content. We conducted two studies to: (1) explore the capabilities of current versions of publicly available GenAI and (2) develop an improved framework to address the limitations we found. Our results showed that GenAI can create math problems at various levels of quality with minimal support, but that providing examples and relevant content results in better quality outputs. This research can help educators decide the ideal way to adopt GenAI in their workflows, to create more effective educational experiences for students.},
	year={2025},
	pdf={reasoning.pdf},
	month= may,
	publisher = {arXiv},
	preview={reasoning.png},
	arxiv = {2505.11899},
	booktitle={International Conference on Artificial Intelligence in Education},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/2505.11899},
	selected = {true}, 
}